{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f9abec-fd1c-46ea-8e75-bf9df651d5e0",
   "metadata": {
    "id": "58f9abec-fd1c-46ea-8e75-bf9df651d5e0"
   },
   "source": [
    "# Example of a Web Scraper\n",
    "We develop a web scraper step by step. The aim is to show the procedure of data collection with the help of a scraper.\n",
    "Individual Wikipedia pages are queried - see also the blog post by Frank Andrade.\n",
    "The text is extracted from the pages. From this, an XML file is generated that can be used as input for the search engine.\n",
    "The pages about the Football World Cup in Wikipedia serve as a basis.\n",
    "\n",
    "Please note\n",
    "Many websites, including Wikipedia, have defense mechanisms against aggressive crawlers/scrapers. Keep this in mind during development and only access Wikipedia\n",
    "if absolutely necessary.\n",
    "\n",
    "Further resources:\n",
    "Scraper vs Crawler: https://medium.com/oncrawl-seo-tips-tricks/an-introduction-to-web-crawler-119a2b492b63\n",
    "Franks Post: https://medium.com/geekculture/yes-you-can-easily-scrape-websites-with-pandas-heres-how-f833157781d5\n",
    "Beautiful Soup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84f834-aeb1-40a6-b8d0-77dcae4a00bf",
   "metadata": {
    "id": "ae84f834-aeb1-40a6-b8d0-77dcae4a00bf"
   },
   "source": [
    "## Getting to know the tools\n",
    "We will spend a few sections getting to know the tools.\n",
    "Then we will build the scraper step by step.\n",
    "We use:\n",
    "\n",
    "BeautifulSoup\n",
    "requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d747a36-7e14-4e13-9a7f-e7c78bc77fb3",
   "metadata": {
    "id": "3d747a36-7e14-4e13-9a7f-e7c78bc77fb3"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "path = '/home/bfh/irsed/daten/FIFA/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab1669-fc34-4738-a20f-b3714e56ac9f",
   "metadata": {
    "id": "0fab1669-fc34-4738-a20f-b3714e56ac9f"
   },
   "source": [
    "### Cooking a soup\n",
    "To generate data with Beautiful Soup, we need a 'soup'. This also requires the lxml parser (already imported with BeautifulSoup).\n",
    "To get the HTML code of a web page, we send a request to this page and get the text as a response.\n",
    "Note: With help(soup) you get a help for the object soup - the command no longer belongs in the finished code, but is practical during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551f8d0a-952c-4698-bb5c-cf039b209cb4",
   "metadata": {
    "id": "551f8d0a-952c-4698-bb5c-cf039b209cb4"
   },
   "outputs": [],
   "source": [
    "web = 'https://en.wikipedia.org/wiki/2014_FIFA_World_Cup'\n",
    "response = requests.get(web)\n",
    "content = response.text\n",
    "soup = BeautifulSoup(content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ffde4-73a4-44cc-a664-bcd65934bd4a",
   "metadata": {
    "id": "b65ffde4-73a4-44cc-a664-bcd65934bd4a"
   },
   "source": [
    "## Getting to know the page structure\n",
    "To make progress with Beautiful Soup, we examine the structure of the source page.\n",
    "To do this, we call it up in the browser and right-click to open the browser's developer tools.\n",
    "There we find regularities that we can use for scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459492f4-2a78-406f-9973-88bbfc7eaa6c",
   "metadata": {
    "id": "459492f4-2a78-406f-9973-88bbfc7eaa6c",
    "outputId": "b9c861e3-ecd5-4a22-a699-12f776f611b0"
   },
   "outputs": [],
   "source": [
    "# Read the title-tag and show the content\n",
    "print(soup.title.string)\n",
    "titel = soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f44fd86-4d26-496d-8ccb-451a86415ea6",
   "metadata": {
    "id": "7f44fd86-4d26-496d-8ccb-451a86415ea6",
    "outputId": "ccc065cc-0fca-4926-9466-26b27aa60b4a"
   },
   "outputs": [],
   "source": [
    "# find all p-tags and count them\n",
    "len(soup.find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e16cd-45f6-4ca9-b38f-55d54aca1db6",
   "metadata": {
    "id": "cc4e16cd-45f6-4ca9-b38f-55d54aca1db6",
    "outputId": "e91cdf32-f995-408b-d603-20c41721fecb"
   },
   "outputs": [],
   "source": [
    "# find and count all h2-tags\n",
    "len(soup.find_all('h2'))\n",
    "\n",
    "# Fetch the tag with index 2 from the list and show it\n",
    "test = soup.find_all('h2')[2]\n",
    "print(test.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303248d-19a5-4486-8d01-2e1dd42d084a",
   "metadata": {
    "id": "3303248d-19a5-4486-8d01-2e1dd42d084a",
    "outputId": "585fa94b-cde8-48dc-b896-43c078baf84b"
   },
   "outputs": [],
   "source": [
    "# We have found all the headlines\n",
    "\n",
    "headlines = soup.find_all('span', class_='mw-heading')\n",
    "headlines = soup.find_all('h2')\n",
    "# print(headlines[1])\n",
    "for h in headlines:\n",
    "    print(h.getText())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5a29e-9bdd-441f-8450-612cd6c7b485",
   "metadata": {
    "id": "0fe5a29e-9bdd-441f-8450-612cd6c7b485",
    "outputId": "2549eb45-ed42-4b24-acb3-ff8209386112"
   },
   "outputs": [],
   "source": [
    "# Now we want to find the text for the headline\n",
    "# We inspect host selection in the browser and find that the text is in p tags that are on\n",
    "# the same level as the h2 tag with the headline we are interested in\n",
    "# We search for all 'sibling tags' of our example tag\n",
    "\n",
    "host_selection = headlines[1]   # fixed for the time being to develop the code\n",
    "print(host_selection)\n",
    "\n",
    "ps = host_selection.find_all_next(\"p\")\n",
    "print(ps)\n",
    "\n",
    "# This is not usable - because the ps also appear, which belong to the following titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f8b7a4-b71f-4681-a16e-cdec4660804c",
   "metadata": {
    "id": "78f8b7a4-b71f-4681-a16e-cdec4660804c",
    "outputId": "f8cae790-5e99-4a39-8a9b-289d20801520"
   },
   "outputs": [],
   "source": [
    "# Obviously the search must be set one level higher\n",
    "# Inspection of the page in the browser shows that the parent element has the class mw-parser-output\n",
    "# search with it\n",
    "\n",
    "body = soup.find_all('div', class_='mw-parser-output')\n",
    "if (len(body) != 1):\n",
    "    print(\"body not found \" + title)\n",
    "\n",
    "alles = body[0]\n",
    "\n",
    "\n",
    "for child in alles.children:\n",
    "    if child.name == 'div':\n",
    "        if child.has_attr(\"class\") and \"mw-heading2\" in child[\"class\"]:\n",
    "            print(\"**********************************\")\n",
    "            h2 = child.find(\"h2\").getText()    \n",
    "            print(h2)\n",
    "    elif child.name == 'p':\n",
    "        print (f'p: {child.name}')\n",
    "        text = \"\"\n",
    "        is_footnote = False;\n",
    "        for elt in child.strings:\n",
    "            if elt.startswith('['):\n",
    "                is_footnote = True\n",
    "            if not is_footnote:\n",
    "                    text += elt\n",
    "            if elt.startswith(']'):\n",
    "                is_footnote = False\n",
    "            \n",
    "        print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d427a1-e013-471f-9b54-982fd2d53472",
   "metadata": {
    "id": "33d427a1-e013-471f-9b54-982fd2d53472"
   },
   "source": [
    "## Create and write XML\n",
    "We need an XML file with the following structure:\n",
    "\n",
    "<pre>\n",
    "&lt;add&gt;\n",
    "  &lt;doc&gt;\n",
    "   &lt;field name=\"xxx\"&gt;yyyyy&lt;/field&gt;\n",
    "   &lt;field name=\"xxx\"&gt;yyyyy&lt;/field&gt;\n",
    "  &lt;/doc&gt;\n",
    "  &lt;doc&gt;\n",
    "   &lt;field name=\"xxx\"&gt;yyyyy&lt;/field&gt;\n",
    "   &lt;field name=\"xxx\"&gt;yyyyy&lt;/field&gt;\n",
    "  &lt;/doc&gt;\n",
    "&lt;/add&gt;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfda46-4fa7-49a1-a8ba-f525938ddadb",
   "metadata": {
    "id": "50dfda46-4fa7-49a1-a8ba-f525938ddadb"
   },
   "outputs": [],
   "source": [
    "xml = BeautifulSoup(features='xml')\n",
    "xml.append(xml.new_tag(\"add\"))\n",
    "add = xml.find('add')\n",
    "document = xml.new_tag('doc')\n",
    "add.append(document)\n",
    "field = xml.new_tag('field', attrs = {'name':'xxxx'})\n",
    "field.string = 'yyyy'\n",
    "field2 = xml.new_tag('field', attrs = {'name':'xxxx'})\n",
    "field2.string = 'yyyy'\n",
    "document.append(field)\n",
    "document.append(field2)\n",
    "document2 = xml.new_tag('doc')\n",
    "add.append(document2)\n",
    "field = xml.new_tag('field', attrs = {'name':'yyyy'})\n",
    "field.string = 'yyyy'\n",
    "field2 = xml.new_tag('field', attrs = {'name':'yyyy'})\n",
    "field2.string = 'yyyy'\n",
    "document2.append(field)\n",
    "document2.append(field2)\n",
    "\n",
    "from bs4.formatter import XMLFormatter\n",
    "formatter = XMLFormatter(indent=0)\n",
    "with open(path+\"test.xml\", \"w\") as f:\n",
    "    f.write(xml.prettify(formatter=formatter))\n",
    "\n",
    "# print(xml.preffify(formatter=formatter))\n",
    "\n",
    "# View and verify the file on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc36409-52a9-439a-a88e-2ecb14732f23",
   "metadata": {
    "id": "0dc36409-52a9-439a-a88e-2ecb14732f23",
    "outputId": "128957aa-ec35-48c1-9610-92109b842e2e"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "print(\"Hello World\")\n",
    "xml = BeautifulSoup(features='xml')\n",
    "add = xml.new_tag('foo')\n",
    "add.string = 'bar&bar'\n",
    "xml.append(add)\n",
    "from bs4.formatter import XMLFormatter\n",
    "formatter = XMLFormatter(indent=0)\n",
    "xml.prettify(formatter=formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c906d8-adec-4397-a48a-df20e50d069d",
   "metadata": {
    "id": "11c906d8-adec-4397-a48a-df20e50d069d"
   },
   "source": [
    "# Find field names\n",
    "- Feldnamen müssen den XML-Regeln genügen und dürfen keine Blanks enthalten.\n",
    "- Wir finden zuerst alle Überschriften heraus und erstellen anschliessend ein Dictionary\n",
    "- Dieses wird später benötigt, wenn die Suchmaschine erstellt wird.\n",
    "\n",
    "The link structure of the Football World Cup pages on Wikipedia is very regular:\n",
    "- https://en.wikipedia.org/wiki/2014_FIFA_World_Cup\n",
    "- https://en.wikipedia.org/wiki/2018_FIFA_World_Cup\n",
    "We can take advantage of this to get all the pages.\n",
    "\n",
    "<pre>\n",
    "years = [1930, 1934, 1938, 1950, 1954, 1958, 1962, 1966, 1970, 1974,\n",
    "         1978, 1982, 1986, 1990, 1994, 1998, 2002, 2006, 2010, 2014,\n",
    "         2018, 2022]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045ded4-2f08-4be8-b8f2-864d73cbfda3",
   "metadata": {
    "id": "4045ded4-2f08-4be8-b8f2-864d73cbfda3",
    "outputId": "9aa4e317-ca83-4a06-cbdc-109b7ceaf614"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# We parse all files and extract all h2\n",
    "def get_headers(year):\n",
    "    web = f'https://en.wikipedia.org/wiki/{year}_FIFA_World_Cup'\n",
    "    response = requests.get(web)\n",
    "    content = response.text\n",
    "    headers = set()\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    bodies = soup.find_all('div', class_='mw-parser-output')\n",
    "    if (len(bodies) != 1):\n",
    "        print(f\"Body inexistent or non-unique: year={year} count={len(bodies)}\")\n",
    "    for body in bodies:\n",
    "        for child in body.children:\n",
    "            if child.name == 'div':\n",
    "                if child.has_attr(\"class\") and \"mw-heading2\" in child[\"class\"]:\n",
    "                    h2 = child.find(\"h2\").getText()    \n",
    "                    print(h2)\n",
    "                    headers.add(h2.lower().replace(\" \",\"_\"))\n",
    "    return headers\n",
    "\n",
    "# Wa already know the years\n",
    "years = [1930, 1934, 1938, 1950, 1954, 1958, 1962, 1966, 1970, 1974,\n",
    "         1978, 1982, 1986, 1990, 1994, 1998, 2002, 2006, 2010, 2014,\n",
    "         2018, 2022]\n",
    "years = [\"2014\"]\n",
    "\n",
    "#Name of the <h1> field - should be the same for all DU\n",
    "fields = {'title'}\n",
    "# Fetch all titles\n",
    "for year in years:\n",
    "    fields = fields.union(get_headers(year))\n",
    "\n",
    "print(fields)\n",
    "\n",
    "# Write all fields nicely sorted and on individual lines in a file\n",
    "fields = list(fields)\n",
    "fields.sort()\n",
    "with open(path+\"fifa_fields.csv\", \"w\") as f:\n",
    "    for field in fields:\n",
    "        f.write(field+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86577cf7-d7ae-4590-ae0d-1d3d96af2bb8",
   "metadata": {
    "id": "86577cf7-d7ae-4590-ae0d-1d3d96af2bb8"
   },
   "source": [
    "# Putting together the script\n",
    "Now we have all the elements we need to put together the finished script.\n",
    "When assembling the output XML, we must take into account the special structure of the input:\n",
    "<pre>\n",
    "h1\n",
    "p\n",
    "h2\n",
    "p\n",
    "h2\n",
    "p\n",
    "</pre>\n",
    "- h1 is read separately\n",
    "- h2 and p are read within the For loop - be careful: the first p belong to the h1\n",
    "- The aim is to create this structure:\n",
    "<pre>\n",
    "&lt;add&gt;\n",
    "  &lt;doc&gt;\n",
    "   &lt;field_name=\"titel\"&gt;Inhalt des h1-Tags&lt;/field&gt;\n",
    "   &lt;field name=\"beschreibung\"&gt;Einleitende Beschreibung&lt;/field&gt;\n",
    "   &lt;field name=\"feld\"&gt;inhalt&lt;/field&gt;\n",
    "   &lt;field name=\"feld\"&gt;inhalt&lt;/field&gt;\n",
    "  &lt;/doc&gt;\n",
    "  &lt;doc&gt;\n",
    "   &lt;field_name=\"titel\"&gt;Inhalt des h1-Tags&lt;/field&gt;\n",
    "   &lt;field name=\"beschreibung\"&gt;Einleitende Beschreibung&lt;/field&gt;\n",
    "   &lt;field name=\"feld\"&gt;inhalt&lt;/field&gt;\n",
    "   &lt;field name=\"feld\"&gt;inhalt&lt;/field&gt;\n",
    "  &lt;/doc&gt;\n",
    "&lt;/add&gt;\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829edd19-483a-43a3-acb0-857f6a2a1cab",
   "metadata": {
    "id": "829edd19-483a-43a3-acb0-857f6a2a1cab",
    "outputId": "27c21d68-c701-4d28-aead-33fa2c6d32db"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def one_document(year, document):\n",
    "    web = f'https://en.wikipedia.org/wiki/{year}_FIFA_World_Cup'\n",
    "    response = requests.get(web)\n",
    "    content = response.text\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    text = \"\"\n",
    "    # Find h1\n",
    "    find_h1(soup, document)\n",
    "    # find all other fields\n",
    "    bodies = soup.find_all('div', class_='mw-parser-output')\n",
    "    if (len(bodies) != 1):\n",
    "        print(f\"Body does not exist or is not unique={year} count={len(bodies)}\")\n",
    "    fieldname = 'description' #first field\n",
    "    for body in bodies:\n",
    "        for child in body.children:\n",
    "            if child.name == 'div':\n",
    "                if child.has_attr(\"class\") and \"mw-heading2\" in child[\"class\"]:\n",
    "                    # print(\"**********************************\")\n",
    "                    create_field(document, fieldname, text)\n",
    "                    text = \"\"\n",
    "                    # print (f'h2: {child.name}')\n",
    "                    h2 = child.find(\"h2\").getText()    \n",
    "                    print(h2)\n",
    "                    fieldname = h2.lower().replace(\" \",\"_\")\n",
    "                    # print(fieldname)\n",
    "            elif child.name == 'p':\n",
    "                #print (f'p: {child.name}')\n",
    "                # text = \"\"\n",
    "                is_footnote = False;\n",
    "                for elt in child.strings:\n",
    "                    if elt.startswith('['):\n",
    "                        is_footnote = True\n",
    "                    if not is_footnote:\n",
    "                            text += elt\n",
    "                    if elt.startswith(']'):\n",
    "                        is_footnote = False\n",
    "                \n",
    "def find_h1(soup, document):\n",
    "    h1 = soup.find('span', class_='mw-page-title-main').string\n",
    "    create_field(document, \"title\", h1)\n",
    "\n",
    "def create_field(document, fieldname, text):\n",
    "    print(fieldname)\n",
    "    if len(text.strip()) > 0:\n",
    "        field = xml.new_tag('field', attrs = {'name':fieldname})\n",
    "        field.string = text\n",
    "        document.append(field)\n",
    "\n",
    "# We will re-use the list of years\n",
    "years = [1930, 1934, 1938, 1950, 1954, 1958, 1962, 1966, 1970, 1974,\n",
    "         1978, 1982, 1986, 1990, 1994, 1998, 2002, 2006, 2010, 2014,\n",
    "         2018, 2022]\n",
    "years = [2014]\n",
    "\n",
    "xml = BeautifulSoup(features='xml')\n",
    "add = xml.new_tag('add')\n",
    "xml.append(add)\n",
    "\n",
    "# Create a new doc-tag per year\n",
    "for year in years:\n",
    "    print(year)\n",
    "    document = xml.new_tag('doc')\n",
    "    create_field(document, \"year\", str(year))   # generate id_field\n",
    "    one_document(year,document)\n",
    "    xml.add.append(document)\n",
    "\n",
    "\n",
    "# write it beautifully\n",
    "from bs4.formatter import XMLFormatter\n",
    "formatter = XMLFormatter(indent=0)\n",
    "with open(path+\"fifa.xml\", \"w\") as f:\n",
    "    output = str(xml)\n",
    "    output.replace('\\n',' ')\n",
    "    print(output)\n",
    "    f.write(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b6e2ed-16c5-4585-a483-3eb06bc2f6fc",
   "metadata": {
    "id": "30b6e2ed-16c5-4585-a483-3eb06bc2f6fc"
   },
   "source": [
    "# Quality\n",
    "The output in the file must be checked and verified manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292509a-1371-4a48-9d12-3718205b4aff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
