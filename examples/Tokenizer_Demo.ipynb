{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE - Byte-Pair Encoding Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Ich bin ein deutscher Text.\",\n",
    "    \"Und ich bin ein etwas längerer deutscher Text.\",\n",
    "    \"Auch ich bin deutschsprachig.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir verwenden den AutoTokenizer aus der HuggingFace Transformers-Library, um den Text in einzelne Zeichen zu zerlegen. Dieser AutoTokenizer ist auch bekannt als gpt2-Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Während der Pre-Tokenisierung berechnen wir die Häufigkeit eines jeden Worts im Corpus.\n",
    "Beachte: Ġ steht für das Leerzeichen innerhalb eines Satzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Ich': 1, 'Ġbin': 3, 'Ġein': 2, 'Ġdeutscher': 2, 'ĠText': 2, '.': 3, 'Und': 1, 'Ġich': 2, 'Ġetwas': 1, 'ĠlÃ¤ngerer': 1, 'Auch': 1, 'Ġdeutschsprachig': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt erstellen wir das Grundvokabular.\n",
    "Gleichzeitig erstellen wir auch unser Alphabet.\n",
    "Im produktiven Kontext würden wir alle Bitfolgen unseres Zeichensatzes nehmen (ASCII, Unicode) und wir würden zusätzlich auch ein Token für 'unknown' erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'A', 'I', 'T', 'U', 'a', 'b', 'c', 'd', 'e', 'g', 'h', 'i', 'l', 'n', 'p', 'r', 's', 't', 'u', 'w', 'x', '¤', 'Ã', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "print(alphabet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir fügen ein weiteres Zeichen hinzu, um ein Textende zu kennzeichnen. So werden die drei Texte in unserem Corpus voneinander abgegrenzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt splitten wir jedes Wort in einzelne Zeichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ich': ['I', 'c', 'h'],\n",
       " 'Ġbin': ['Ġ', 'b', 'i', 'n'],\n",
       " 'Ġein': ['Ġ', 'e', 'i', 'n'],\n",
       " 'Ġdeutscher': ['Ġ', 'd', 'e', 'u', 't', 's', 'c', 'h', 'e', 'r'],\n",
       " 'ĠText': ['Ġ', 'T', 'e', 'x', 't'],\n",
       " '.': ['.'],\n",
       " 'Und': ['U', 'n', 'd'],\n",
       " 'Ġich': ['Ġ', 'i', 'c', 'h'],\n",
       " 'Ġetwas': ['Ġ', 'e', 't', 'w', 'a', 's'],\n",
       " 'ĠlÃ¤ngerer': ['Ġ', 'l', 'Ã', '¤', 'n', 'g', 'e', 'r', 'e', 'r'],\n",
       " 'Auch': ['A', 'u', 'c', 'h'],\n",
       " 'Ġdeutschsprachig': ['Ġ',\n",
       "  'd',\n",
       "  'e',\n",
       "  'u',\n",
       "  't',\n",
       "  's',\n",
       "  'c',\n",
       "  'h',\n",
       "  's',\n",
       "  'p',\n",
       "  'r',\n",
       "  'a',\n",
       "  'c',\n",
       "  'h',\n",
       "  'i',\n",
       "  'g']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Funktion berechnet die Häufigkeit eines jeden Tokenpaars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wenden die Funktion an und schauen das Ergebnis an: 'c' kommt am häufigsten vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'c'): 1\n",
      "('c', 'h'): 8\n",
      "('Ġ', 'b'): 3\n",
      "('b', 'i'): 3\n",
      "('i', 'n'): 5\n",
      "('Ġ', 'e'): 3\n",
      "('e', 'i'): 2\n",
      "('Ġ', 'd'): 3\n",
      "('d', 'e'): 3\n",
      "('e', 'u'): 3\n",
      "('u', 't'): 3\n",
      "('t', 's'): 3\n",
      "('s', 'c'): 3\n",
      "('h', 'e'): 2\n",
      "('e', 'r'): 4\n",
      "('Ġ', 'T'): 2\n",
      "('T', 'e'): 2\n",
      "('e', 'x'): 2\n",
      "('x', 't'): 2\n",
      "('U', 'n'): 1\n",
      "('n', 'd'): 1\n",
      "('Ġ', 'i'): 2\n",
      "('i', 'c'): 2\n",
      "('e', 't'): 1\n",
      "('t', 'w'): 1\n",
      "('w', 'a'): 1\n",
      "('a', 's'): 1\n",
      "('Ġ', 'l'): 1\n",
      "('l', 'Ã'): 1\n",
      "('Ã', '¤'): 1\n",
      "('¤', 'n'): 1\n",
      "('n', 'g'): 1\n",
      "('g', 'e'): 1\n",
      "('r', 'e'): 1\n",
      "('A', 'u'): 1\n",
      "('u', 'c'): 1\n",
      "('h', 's'): 1\n",
      "('s', 'p'): 1\n",
      "('p', 'r'): 1\n",
      "('r', 'a'): 1\n",
      "('a', 'c'): 1\n",
      "('h', 'i'): 1\n",
      "('i', 'g'): 1\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f\"{key}: {pair_freqs[key]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir suchen das häufigste Tokenpaar: 'ch' kommt am häufigsten vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('c', 'h') 8\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir verschmelzen 'c' und 'h' zu 'ch' und fügen das neue Token dem Vokabular hinzu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = {(\"c\", \"h\"): \"ch\"}\n",
    "vocab.append(\"ch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Funktion wendet den gezeigten Merge-Step auf das Split-Directory an. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schauen wir das Ergebnis unseres ersten Merges an:\n",
    "Aus 'I','c','h' wurde neu 'I','ch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ich': ['I', 'ch'],\n",
       " 'Ġbin': ['Ġ', 'b', 'i', 'n'],\n",
       " 'Ġein': ['Ġ', 'e', 'i', 'n'],\n",
       " 'Ġdeutscher': ['Ġ', 'd', 'e', 'u', 't', 's', 'ch', 'e', 'r'],\n",
       " 'ĠText': ['Ġ', 'T', 'e', 'x', 't'],\n",
       " '.': ['.'],\n",
       " 'Und': ['U', 'n', 'd'],\n",
       " 'Ġich': ['Ġ', 'i', 'ch'],\n",
       " 'Ġetwas': ['Ġ', 'e', 't', 'w', 'a', 's'],\n",
       " 'ĠlÃ¤ngerer': ['Ġ', 'l', 'Ã', '¤', 'n', 'g', 'e', 'r', 'e', 'r'],\n",
       " 'Auch': ['A', 'u', 'ch'],\n",
       " 'Ġdeutschsprachig': ['Ġ',\n",
       "  'd',\n",
       "  'e',\n",
       "  'u',\n",
       "  't',\n",
       "  's',\n",
       "  'ch',\n",
       "  's',\n",
       "  'p',\n",
       "  'r',\n",
       "  'a',\n",
       "  'ch',\n",
       "  'i',\n",
       "  'g']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = merge_pair(\"c\", \"h\", splits)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir legen eine Vokabulargröße fest und wiederholen alles bis diese Größe erreicht ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('c', 'h'): 'ch', ('e', 'r'): 'er', ('Ġ', 'b'): 'Ġb', ('Ġb', 'in'): 'Ġbin', ('Ġ', 'e'): 'Ġe', ('Ġ', 'd'): 'Ġd', ('Ġd', 'e'): 'Ġde', ('Ġde', 'u'): 'Ġdeu', ('Ġdeu', 't'): 'Ġdeut', ('Ġdeut', 's'): 'Ġdeuts', ('Ġdeuts', 'ch'): 'Ġdeutsch', ('Ġe', 'in'): 'Ġein', ('Ġdeutsch', 'er'): 'Ġdeutscher', ('Ġ', 'T'): 'ĠT', ('ĠT', 'e'): 'ĠTe', ('ĠTe', 'x'): 'ĠTex', ('ĠTex', 't'): 'ĠText', ('Ġ', 'i'): 'Ġi', ('Ġi', 'ch'): 'Ġich', ('I', 'ch'): 'Ich', ('U', 'n'): 'Un', ('Un', 'd'): 'Und', ('Ġe', 't'): 'Ġet', ('Ġet', 'w'): 'Ġetw'}\n"
     ]
    }
   ],
   "source": [
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', '.', 'A', 'I', 'T', 'U', 'a', 'b', 'c', 'd', 'e', 'g', 'h', 'i', 'l', 'n', 'p', 'r', 's', 't', 'u', 'w', 'x', '¤', 'Ã', 'Ġ', 'ch', 'er', 'Ġb', 'Ġbin', 'Ġe', 'Ġd', 'Ġde', 'Ġdeu', 'Ġdeut', 'Ġdeuts', 'Ġdeutsch', 'Ġein', 'Ġdeutscher', 'ĠT', 'ĠTe', 'ĠTex', 'ĠText', 'Ġi', 'Ġich', 'Ich', 'Un', 'Und', 'Ġet', 'Ġetw']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ich', 'Ġ', 's', 'p', 'r', 'e', 'ch', 'e', 'Ġdeutsch']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"Ich spreche deutsch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siehe auch: https://platform.openai.com/tokenizer "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aie-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
